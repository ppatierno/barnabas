Configures additional connectors for Kafka Connect deployments.

=== `output`

In order to build new container images with additional connector plugins, Strimzi requires a container registry where the images can be pushed to, stored and pulled from.
Strimzi does not run its own container registry, so users need to provide their own registry.
Strimzi is able to use private container registries as well as public registries such as link:https://quay.io/[Quay^] or link:https://hub.docker.com//[Docker Hub^].
The container registry is configured in the `.spec.build.output` section of the `KafkaConnect` custom resource.
The `output` configuration supports two types: `docker` and `imagestream`.
The `output` section is required.

.Using Docker registry

To use a Docker registry, you have to specify the `type` as `docker`, and the `image` field with the full name of the new container image.
The full name has to include:

* The address of the registry.
* Port number if it is listening on non-standard port
* The tag of the new container image

Examples of valid image names are:

* `docker.io/my-org/my-image/my-tag`
* `quay.io/my-org/my-image/my-tag`
* `image-registry.image-registry.svc:5000/myproject/kafka-connect-build:latest`

Different Kafka Connect deployments should use a different image (for example at least with different tags).

If the registry requires authentication, the field `pushSecret` can be used to set a name of the Secret with the registry credentials.
The Secret should use the `kubernetes.io/dockerconfigjson` type and contain the `.dockerconfigjson` file with the Docker credentials.
For more details see link:https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials[Create a Secret based on existing Docker credentials^]

[source,yaml,subs=attributes+,options="nowrap"]
.Example `output` configuration
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  #...
  build:
    output:
      type: docker # <1>
      image: my-registry.io/my-org/my-connect-cluster:latest # <2>
      pushSecret: my-registry-credentials # <3>
  #...
----
<1> The type of the output which should be used by Strimzi. (Required)
<2> The full name of the image which will be used including the repository and tag. (Required)
<3> Name of the secret with the container registry credentials. (Optional)

.Using OpenShift ImageStream

Alternatively, you can also use OpenShift ImageStream to store the new container image.
The ImageStream has to be created manually before deploying the Kafka Connect.
To use OpenShift ImageStream, you have to set the `type` to `imagestream`, and the `image` field should contain the name of the ImageStream and the tag which should be used.
For example `my-connect-image-stream:latest`.

[source,yaml,subs=attributes+,options="nowrap"]
.Example `output` configuration
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  #...
  build:
    output:
      type: imagestream # <1>
      imageStream: image-stream-name # <2>
  #...
----
<1> The type of the output which should be used by Strimzi. (Required)
<2> The name of the ImageStream and tag. (Required)

=== `plugins`

The connector plugins which will be added to the container image are configured in the `.spec.build.plugins` section of the `KafkaConnect` custom resource.
Each connector plugin needs to have a name which is unique within the Kafka Connect deployment.
Additionally, it contains a list of artifacts which it consists of.
These artifacts will be downloaded by Strimzi, added to the new container image, and used in the Kafka Connect deployment.
The connector plugin artifacts can also include additional components such as (de)serializers and so on.
Each connector plugin will be downloaded into a separate directory so that the different connectors and their dependencies are properly sand-boxed.
The `plugins` section is required.
Each plugin has to be configured with at least one `artifact`.

[source,yaml,subs=attributes+,options="nowrap"]
.Example `plugins` configuration with two connector plugins
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  #...
  build:
    output:
      #...
    plugins: # <1>
      - name: debezium-postgres-connector
        artifacts:
          - type: tgz
            url: https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.3.1.Final/debezium-connector-postgres-1.3.1.Final-plugin.tar.gz
            sha512sum: 962a12151bdf9a5a30627eebac739955a4fd95a08d373b86bdcea2b4d0c27dd6e1edd5cb548045e115e33a9e69b1b2a352bee24df035a0447cb820077af00c03
      - name: camel-telegram
        artifacts:
          - type: tgz
            url: https://repo.maven.apache.org/maven2/org/apache/camel/kafkaconnector/camel-telegram-kafka-connector/0.7.0/camel-telegram-kafka-connector-0.7.0-package.tar.gz
            sha512sum: a9b1ac63e3284bea7836d7d24d84208c49cdf5600070e6bd1535de654f6920b74ad950d51733e8020bf4187870699819f54ef5859c7846ee4081507f48873479
  #...
----
<1> List of connector plugins and their artifacts which should be downloaded. (Required)

Currently, Strimzi supports two types of artifacts:
* Downloading JAR files and using them directly
* Downloading TGZ archives and unpacking them

IMPORTANT: Strimzi does not do any security scanning of the downloaded artifacts.
For security reasons, you should first verify the artifacts manually and configure the checksum verification to make sure the same artifact is used in the automated build and in the Kafka Connect deployment.

.Using JAR artifacts

The JAR artifacts represent a resource which will be downloaded and added to the container image.
It is intended to be mainly used for downloading JAR files.
It can be also used to download other file types.
To use a JAR artifacts, you have to set the `type` field to `jar` and use the `url` to specify the URL from which it should be downloaded.

Additionally, you can specify SHA-512 checksum of the file which will be downloaded.
If specified, Strimzi will verify the checksum of the artifact while building the new container image.

[source,yaml,subs=attributes+,options="nowrap"]
.Example of a JAR artifact
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  #...
  build:
    output:
      #...
    plugins:
      - name: my-plugin
        artifacts:
          - type: jar # <1>
            url: https://my-domain.tld/my-jar.jar # <2>
            sha512sum: 589...ab4 # <3>
          - type: jar
            url: https://my-domain.tld/my-jar2.jar
  #...
----
<1> Type of the artifact. (Required)
<2> The URL from which the artifact will be downloaded. (Required)
<3> The SHA-512 checksum to verify the artifact. (Optional)

.Using TGZ artifacts

The TGZ artifacts can be used to download TAR archives compressed using the Gzip compression.
The TGZ archive can contain the whole Kafka Connect connector even if it consists of multiple different files.
It will be automatically downloaded and unpacked by Strimzi while building the new container image.
To use a TGZ artifacts, you have to set the `type` field to `tgz` and use the `url` to specify the URL from which the TGZ archive should be downloaded.

Additionally, you can specify SHA-512 checksum of the archive which will be downloaded.
If specified, Strimzi will verify the checksum before unpacking it and building the new container image.

[source,yaml,subs=attributes+,options="nowrap"]
.Example of a JAR artifact
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  #...
  build:
    output:
      #...
    plugins:
      - name: my-plugin
        artifacts:
          - type: tgz # <1>
            url: https://my-domain.tld/my-connector-archive.jar # <2>
            sha512sum: 158...jg10 # <3>
  #...
----
<1> Type of the artifact. (Required)
<2> The URL from which the archive will be downloaded. (Required)
<3> The SHA-512 checksum to verify the archive. (Optional)